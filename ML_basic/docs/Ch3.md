# K-NN회귀, 선형 회귀

## K-NN 회귀
K-NN 분류에서는 최근접 K개의 원소의 class를 따라갔다면 K-NN회귀에서는 최근접 K개의 원소의 값을 평균내어 예측한다.  
단순히 평균내는 것이 아니라 가중평균을 낼 수도 있다.

sikitlearn에서는 K-NN 회귀의 score로 R^2을 제공한다. R^2은 전체 분산 중 회귀 결과가 설명하는 분산(1 - sum(잔차^2)/분산 = 1 - SSE / SST)을 의미한다.

## 선형 회귀
선형 회귀는 주어진 데이터를 가장 잘 설명하는 회귀 직선을 찾는 것을 목적으로 한다.  
가장 잘 설명한다는 것은 여러 기준으로 판단할 수 있지만, 보통 MSE error를 많이 쓴다.  

### 일변수 선형 회귀
일변수 선형 회귀는 독립변수와 종속변수가 각각 하나인 경우이다.  
최소제곱법을 사용해 식 $w = Cov(x, y) / Var(x),b = \bar y - w\bar x$ 과 같이 기울기(weight)와 절변(bias, intersection)을 바로 구할 수 있다.  

### 다변수 선형 회귀
다변수 선형 회귀는 일변수와 같이 간단한 관계식은 존재하지 않는다.  
만약 $(X'^T X')$이 가역이면 (X에 bias항이 추가된 것)  
$\theta' = (X'^T X')^{-1}X'^T y$ 이다.  
그렇지 않을 경우 SVD(특잇값 분해)를 이용해 구해야 한다.

# 특성 공학과 규제
특성 공학은 기본 feature를 조합해 새로운 feature을 만들어내는 것을 의미한다.  
과도하게 특성 공학을 하게 되면 과적합이 발생할 수 있고 이런 경우 (혹은 다른 경우에도)규제를 통해 모델 성능을 개선할 수 있다.  
규제는 오차 함수에 기울기 벡터의 L2노름을 추가한 릿지 회귀와 L1노름을 추가한 라쏘 회귀가 대표적이다.  
***규재는 크기에 비례하는 제약을 걸어주는 것이기 때문에 정규화를 먼저 해주는 것이 좋다***  
규제가 적용된 릿지 회귀나 라쏘 회귀는 바로 값을 계산하는 방법이 없기 때문에 gradient descent를 사용해야 한다.

## 특성 공학
특성 공학을 통해 feature x에서 x^2, x^3등을 추가해 넣어줄 수 있다.  
이런 경우 데이터가 비선형적인 특성을 보이더라도 적절한 fitting이 가능해진다.

## 릿지 회귀
릿지 회귀는 오차에 기울기 벡터의 L2노름을 추가한다.  
$Loss(w, b) = MSE + \alpha * ||w||_2$

## 라쏘 회귀
라쏘 회귀는 오차에 기울기 벡터의 L1노름을 추가한다.  
$Loss(w, b) = MSE +\alpha * ||w||_1$